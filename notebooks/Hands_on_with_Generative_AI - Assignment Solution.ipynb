{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1NPvzO8kwkcvhGP9hB1pK9Y7mrUDerkNu","timestamp":1714967678076}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Hands-On with Generative AI Assignment\n","\n","Here you will gain hands-on experience working with GPT language models via the OpenAI API. Specifically, you will:\n","1. Create an OpenAI account and obtain an API key.\n","2. Try generating outputs from several different models available via the API.\n","3. Gain a first exposure to prompt engineering by experimenting with using multiple prompts for a given task."],"metadata":{"id":"nx0qiGWO7qWV"}},{"cell_type":"markdown","source":["---\n","## Problem 1\n","\n","Create an [OpenAI account](https://openai.com/index/openai-api), then generate an API key."],"metadata":{"id":"pVACWFAT7_k6"}},{"cell_type":"markdown","source":["## Solution\n","\n","After logging into your account, go to [https://platform.openai.com/api-keys](https://platform.openai.com/api-keys). Click the \"Create new secret key\" button to create a new API key. Copy your newly created API key upon creation."],"metadata":{"id":"hclHhBFFofjB"}},{"cell_type":"markdown","source":["---\n","## Problem 2\n","\n","Use the OpenAI API to produce responses to a given prompt using two different models. First use the GPT-3.5 Turbo model to evaluate the prompt, then use the GPT-4 Turbo model to evaluate the same prompt.\n","\n","For your prompt, try asking the models to perform the following task which requires a bit of reasoning to solve:\n","```\n","Andrew is free from 11 am to 3 pm, Joanne is free from noon to 2 pm and then 3:30 pm to 5 pm.\n","Hannah is available at noon for half an hour, and then 4 pm to 6 pm.\n","What are some options for start times for a 30 minute meeting for Andrew, Hannah, and Joanne?\n","```\n","Compare the quality of the outputs that are obtained from the two different models."],"metadata":{"id":"ycWJVTEyoibf"}},{"cell_type":"markdown","source":["## Solution\n","\n","To complete this problem we will need to perform the following steps:\n","1. Install the `openai` Python package.\n","2. Create an OpenAI client using your API key. In these solutions we load our key from Google drive to avoid hard-coding the API key in our solution.\n","3. Pass the prompt above to the `chat.completions` API using the `gpt-3.5-turbo` model.\n","4. Pass the prompt above to the `chat.completions` API using the `gpt-4-turbo` model.\n","5. Compare the outputs obtained by both models."],"metadata":{"id":"a6WBn6swqhHV"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qp2JAR0-28Kt","executionInfo":{"status":"ok","timestamp":1739169229962,"user_tz":-330,"elapsed":4734,"user":{"displayName":"Utkarsh","userId":"17583634558375598402"}},"outputId":"423b2aa6-e52b-48a0-8c06-1faff4d4e582"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.61.1)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.8.2)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.10.6)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n","Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.12.2)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n"]}],"source":["# Install the openai package\n","!pip install openai"]},{"cell_type":"code","source":["# Import packages that you will use for accessing the OpenAI API\n","import json\n","from google.colab import drive\n","from openai import OpenAI"],"metadata":{"id":"qHqi4fZW3LL5","executionInfo":{"status":"ok","timestamp":1739169232179,"user_tz":-330,"elapsed":2214,"user":{"displayName":"Utkarsh","userId":"17583634558375598402"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Mount Google Drive.\n","# We will get our OpenAI API key from a file that we stored in Google Drive.\n","drive.mount(\"/content/gdrive\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tiOTaZKz3j32","executionInfo":{"status":"ok","timestamp":1739169445118,"user_tz":-330,"elapsed":21593,"user":{"displayName":"Utkarsh","userId":"17583634558375598402"}},"outputId":"4c9559dc-cff4-4ac5-d2a6-9f5d664f028c"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["# Read in API key\n","with open(\"/content/gdrive/MyDrive/keys.json\", \"r\") as f:\n","  api_key = json.loads(f.read())[\"api_key\"]"],"metadata":{"id":"9wKV2nPO3Omo","executionInfo":{"status":"ok","timestamp":1739169445629,"user_tz":-330,"elapsed":507,"user":{"displayName":"Utkarsh","userId":"17583634558375598402"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Create an OpenAI client\n","client = OpenAI(api_key=api_key)"],"metadata":{"id":"9SJd5wNw3RJ3","executionInfo":{"status":"ok","timestamp":1739169449627,"user_tz":-330,"elapsed":221,"user":{"displayName":"Utkarsh","userId":"17583634558375598402"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Generating response from gpt-3.5-turbo\n","\n","prompt = \"\"\"\n","Andrew is free from 11 am to 3 pm, Joanne is free from noon to 2 pm and then 3:30 pm to 5 pm.\n","Hannah is available at noon for half an hour, and then 4 pm to 6 pm.\n","What are some options for start times for a 30 minute meeting for Andrew, Hannah, and Joanne?\n","\"\"\"\n","\n","# Note the use of the \"temperature\" parameter...\n","# ...this allows us to generate reproducable outputs\n","openai_response = client.chat.completions.create(\n","    model = 'gpt-3.5-turbo',\n","    messages = [{'role': 'user', 'content': prompt}],\n","    temperature = 0\n",")\n","\n","# Print the response\n","print(openai_response.choices[0].message.content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J3oPP_fX4VYP","executionInfo":{"status":"ok","timestamp":1739169452302,"user_tz":-330,"elapsed":1529,"user":{"displayName":"Utkarsh","userId":"17583634558375598402"}},"outputId":"f998906c-bc8b-47ae-a369-8db3c2395dec"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Some options for start times for a 30-minute meeting for Andrew, Hannah, and Joanne could be:\n","- 12:00 pm\n","- 12:30 pm\n","- 4:00 pm\n"]}]},{"cell_type":"code","source":["# Now generating response from gpt-4-turbo\n","openai_response = client.chat.completions.create(\n","    model = 'gpt-4-turbo',\n","    messages = [{'role': 'user', 'content': prompt}],\n","    temperature = 0\n",")\n","\n","# Print the response\n","print(openai_response.choices[0].message.content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":382},"id":"RRlgbQFPr0aX","executionInfo":{"status":"error","timestamp":1739169456387,"user_tz":-330,"elapsed":295,"user":{"displayName":"Utkarsh","userId":"17583634558375598402"}},"outputId":"b49b776d-b7c4-4a37-f861-8d537595c799"},"execution_count":7,"outputs":[{"output_type":"error","ename":"PermissionDeniedError","evalue":"Error code: 403 - {'error': {'message': 'Project `proj_6amWdXZiZG5XHtR3u182LRLv` does not have access to model `gpt-4-turbo`', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPermissionDeniedError\u001b[0m                     Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-50d9110a00d9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Now generating response from gpt-4-turbo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m openai_response = client.chat.completions.create(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'gpt-4-turbo'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmessages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'role'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'user'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'content'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtemperature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    861\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m    862\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    864\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m             body=maybe_transform(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1281\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m         )\n\u001b[0;32m-> 1283\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m     def patch(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    958\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    961\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m         return self._process_response(\n","\u001b[0;31mPermissionDeniedError\u001b[0m: Error code: 403 - {'error': {'message': 'Project `proj_6amWdXZiZG5XHtR3u182LRLv` does not have access to model `gpt-4-turbo`', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}"]}]},{"cell_type":"markdown","source":["**Comparison:** GPT-4 produced a higher quality output in this case. GPT-3.5 incorrectly states that 3:30 pm - 4:00 pm and 4:00 pm - 4:30 pm are options for the meeting. GPT-4 correctly reasons that 12-12:30 is the only feasible time."],"metadata":{"id":"JT660kbsub7G"}},{"cell_type":"markdown","source":["---\n","## Problem 3\n","Here you will see an example of how the prompting strategy can impact the quality of the output produced. In this problem you will again apply GPT-3.5 to the reasoning task from Problem 2. However, unlike in Problem 2, modify your prompt to include some general step-by-step instructions for solving a scheduling task like the one given. Can you produce a prompt that enables GPT-3.5 to arrive at a correct answer?"],"metadata":{"id":"YLkPtSavLIQF"}},{"cell_type":"markdown","source":["## Solution\n","\n","We will modify our prompt by including the following general step-by-step instructions for solving a scheduling task:\n","```\n","You will be given a complex scheduling task below. Solve this task step-by-step by:\n","- First identifying all time slots that each individual is available.\n","- Then finding all time slots that all individuals are available.\n","- Then selecting a time slot from among those where all individuals are available.\n","```\n","Note that these instructions are fairly generic, and don't provide any hints specific to the details of the given scheduling problem. However, this additional instruction appears sufficient to guide GPT-3.5 to a correct answer.\n"],"metadata":{"id":"x5YFHsonu50b"}},{"cell_type":"code","source":["# Generating response from gpt-3.5-turbo\n","\n","prompt = \"\"\"\n","You will be given a complex scheduling task below. Solve this task step-by-step by:\n","- First identifying all time slots that each individual is available.\n","- Then finding all time slots that all individuals are available.\n","- Then selecting a time slot from among those where all individuals are available.\n","\n","Task: Andrew is free from 11 am to 3 pm, Joanne is free from noon to 2 pm and then 3:30 pm to 5 pm.\n","Hannah is available at noon for half an hour, and then 4 pm to 6 pm.\n","What are some options for start times for a 30 minute meeting for Andrew, Hannah, and Joanne?\n","\"\"\"\n","\n","openai_response = client.chat.completions.create(\n","    model = 'gpt-3.5-turbo',\n","    messages = [{'role': 'user', 'content': prompt}],\n","    temperature = 0\n",")\n","\n","# Print the response\n","print(openai_response.choices[0].message.content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GHHImRTpvp7i","executionInfo":{"status":"ok","timestamp":1715087479507,"user_tz":240,"elapsed":4035,"user":{"displayName":"Randy Cogill","userId":"11828508659675820150"}},"outputId":"c7f1a46a-ef43-4af4-aa16-32f0e476eae4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["First, let's identify all the time slots that each individual is available:\n","\n","Andrew: 11 am - 3 pm\n","Joanne: 12 pm - 2 pm, 3:30 pm - 5 pm\n","Hannah: 12 pm - 12:30 pm, 4 pm - 6 pm\n","\n","Next, let's find all time slots where all individuals are available:\n","- The only overlapping time slot where all three individuals are available is from 12 pm to 12:30 pm.\n","\n","Therefore, the best option for a 30-minute meeting for Andrew, Hannah, and Joanne would be to schedule it at 12 pm.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"BG7UhaPuv12t"},"execution_count":null,"outputs":[]}]}