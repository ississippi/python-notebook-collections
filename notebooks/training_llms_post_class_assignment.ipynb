{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Training LLMs - Fine Tuning Language Model on Semantic Tasks\n","\n","In this assignment we are going to fine tune an off the shelf pre-trained language model to understand semantic similarity. To do this we are going to use the Glue - MSRC dataset provided by microsoft to understand such semantics.\n","\n","The goal for this assignment is to take an off the shelf language model that is already pre-trained and fine tune it on the task understanding semantic analysis. The languge model that we are going to use is the base `roberta` model that is larger than the original base `bert` model. There are other modifications that `roberta` did to enhance `bert` such as dynamic masking, the removal of the next sentence prediction task, as well as a more enhanced tokenzer.\n","\n","This assignment will walk you through the steps needed to accomplish this task. You will be asked to fill in the various code blocks as we progress through the notebook. Please view the comments to monitor which code blocks to complete."],"metadata":{"id":"rYDJ6PpgKDdq"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"09PLyzi4fOoT"},"outputs":[],"source":["# let's first install the various libraries that we'll\n","# need for this assignment\n","!pip install -q peft datasets evaluate\n"]},{"cell_type":"code","source":["! pip install transformers[torch]"],"metadata":{"id":"RicdRmGtDy69"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# we are going to import\n","# the various methods and classes that we will use\n","# throughout the notebook.\n","from transformers import (\n","    AutoModelForSequenceClassification,\n","    AutoTokenizer,\n","    DataCollatorWithPadding,\n","    TrainingArguments,\n","    Trainer,\n",")\n","from peft import (\n","    get_peft_config,\n","    get_peft_model,\n","    LoraConfig,\n","    TaskType\n","    )\n","from datasets import load_dataset\n","import evaluate\n","import torch\n","import numpy as np\n","\n","# we are going to pull in the RoBERTA model\n","# which is a modification of BERT\n","model_name_or_path = \"roberta-base\"\n","# this is going to be the specific dataset that\n","# we are pulling from load dataset.\n","# this data allows us to understand\n","# semantic similarity between documents.\n","task = \"mrpc\" # microsoft research paraphrase corpus"],"metadata":{"id":"mXwAonRyfW_N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# let's load in the glue dataset with the MRPC task\n","dataset = load_dataset(\"glue\", task)"],"metadata":{"id":"aBQp_9HdY1UI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# let's view the data set as a whole\n","dataset"],"metadata":{"id":"EZgc8nZ3Y1Ra"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# look at a few records of the train dataset.\n","# label refers to if those sentences are indeed\n","# similar\n","\n","## code here\n"],"metadata":{"id":"brU6NJ-TZEUM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As is typical when fine tuning language models, we need to create a function that will keep track of metrics while training. In order to do this we are going to use the native metric that is seen in the GLEU dataset creation.\n","\n","For more information on the GLUE Metric and Datasets, please view this link: [GLEU](https://huggingface.co/spaces/evaluate-metric/glue)."],"metadata":{"id":"Kc_MOFpvdnS8"}},{"cell_type":"code","source":["# at this point we are going to load in the metric\n","# that we should be using when evaluating the MRPC dataset.\n","# we will use this as part of computing metrics\n","metric = evaluate.load(\"glue\", task)"],"metadata":{"id":"-Qo854u6gDIw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# let's visualize what this metric\n","# looks like.\n","metric"],"metadata":{"id":"fVsNQIhSZdPf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# we can see an example here\n","# between references and predictions\n","# this metric is how we will account for\n","# the training and validation loss during\n","# model training\n","references = [0, 1]\n","predictions = [1, 1]\n","results = metric.compute(predictions=predictions, references=references)\n","print(results)"],"metadata":{"id":"-YLQmEPZZwly"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# go ahead and write a compute_metrics\n","# function that will take an eval_pred\n","# object and return the metric calculation\n","# of the predictions vs the labels.\n","\n","## code here\n"],"metadata":{"id":"7Ym3RWcWgIvQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load in the requisite tokenizer for the RoBERTA model\n","tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, padding_side=\"right\")\n","if getattr(tokenizer, \"pad_token_id\") is None:\n","    tokenizer.pad_token_id = tokenizer.eos_token_id\n","\n","# write a function that will take in a batch (or example)\n","# and tokenize both the first sentence and second sentence\n","# make sure to truncate the text and don't worry about the max length for now.\n","\n","## code here\n"],"metadata":{"id":"YWM_QPGvgMEB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# take your just written tokenize function\n","# and tokenize the entire dataset that we\n","# pulled in at the beginning\n","\n","## code here\n","tokenized_datasets = dataset.map(\n","    tokenize_function,\n","    batched=True,\n","    remove_columns=[\"idx\", \"sentence1\", \"sentence2\"],\n",")\n","\n","# afterwards, rename the \"label\" feature as \"labels\"\n","\n","## code here\n"],"metadata":{"id":"c3jQdyjggPPx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# view the first few examples of\n","# your tokenized data set to see what it looks\n","# like.\n","tokenized_datasets['train'][0]"],"metadata":{"id":"co6nVJa4Ecfs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# so that you van view get the input ids from any\n","# example that you choose, and run it through the following code,\n","# what do you notice?\n","example_input_ids = tokenized_datasets['train'][0]['input_ids']\n","tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(example_input_ids))"],"metadata":{"id":"mJbKKu6t_azH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# let's now make a DataCollator object will dynamically pad\n","# our inputs using the tokenizer in question.\n","# ideally we want to pad to the longest sentences that we see in question.\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=\"longest\")"],"metadata":{"id":"28yt8X0lgUak"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Fine Tuning Languge Model\n","Let's first fine tune the full language model. We will then compare it to fine tuning on the PEFT version and notice any major differences."],"metadata":{"id":"YJ1i7iGmi-So"}},{"cell_type":"code","source":["# pull in the RoBERTA model\n","# remember to use AutoModelForSequenceClassification\n","# class because we are going to be classifying on a known label.\n","model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, return_dict=True)"],"metadata":{"id":"AegNuYUGi99h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## find the number of trainable parameters that this model\n","## will use when fine tuning\n","\n","## code here\n"],"metadata":{"id":"SDfXVUyNDBMj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## using the TrainingArguments class\n","## choose the best hyperparameters for\n","## fine tuning the languge model.\n","## NOTE: It may help because of the size of the\n","## Roberta model to use logging_steps around 100.\n","\n","## code here\n","training_args ="],"metadata":{"id":"4rc9mFzJi90a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# put your model, training args, datasets,\n","# tokenizer, data collator, and metrics into a Trainer object\n","# and then begin the fine tuning process\n","\n","## code here\n","trainer =\n","\n","# train the model here\n","## code here\n"],"metadata":{"id":"5U-D2kj3i9By"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n","import matplotlib.pyplot as plt\n","# here is a plot confusion matrix from before\n","# lets use it to plot a confusion matrix\n","# of our labels\n","def plot_confusion_matrix(y_preds, y_true, labels):\n","    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n","    fig, ax = plt.subplots(figsize=(6, 6))\n","    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n","    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n","    plt.title(\"Normalized confusion matrix\")\n","    plt.show()\n","\n","# get the predictions and test data here\n","y_preds = np.argmax(trainer.predict(tokenized_datasets[\"test\"]).predictions, axis=1)\n","y_test = tokenized_datasets[\"test\"]['labels']"],"metadata":{"id":"CbNIPwFLFDau"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot the confusion matrix\n","## code here\n"],"metadata":{"id":"FH26lRMwFS8W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Train the LoRA variation of the RoBERTA Model\n","In this section we are going to fine tune the language model using the LoRA configuration. It will follow a similar procedure as done before."],"metadata":{"id":"7S8fcTssliWl"}},{"cell_type":"code","source":["# let us now get the LoRA confirguration\n","# get the lora configuration of the model\n","# in order to do this use lora_config and choose the appropriate rank\n","\n","## code here\n","\n"],"metadata":{"id":"S2aKQKSVgYUd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# now create the pretained model using the RoBERTA path\n","# and wrap it around the lora configuration\n","# afterwards print out the number of trainable parameters\n","# what do you notice with the original roberta model?\n","\n","## code here\n"],"metadata":{"id":"1Bh1xtUlgg2Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# in a similar way as before,\n","# write out the training arguments that you wish\n","# to use for the lora configuration of Roberta.\n","\n","## code here\n","training_args ="],"metadata":{"id":"Nv6HNJ1fgsnM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# similarly as before write down the Trainer\n","# object and with your training arguments, lora model\n","# datasets, tokenizer, data collator, and compute metrics\n","\n","## code here\n","trainer =\n","\n","# train the model\n","## code here\n"],"metadata":{"id":"NXMGYTQkg7dU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get the training predictions\n","# as well as the test target outputs\n","# lastly plot the confusion matric\n","\n","## code here\n","y_preds = np.argmax(trainer.predict(tokenized_datasets[\"test\"]).predictions, axis=1)\n","y_test = tokenized_datasets[\"test\"]['labels']\n","labels = [\"not equivalent\", \"equivalent\"]\n","plot_confusion_matrix(y_preds, y_test, labels)"],"metadata":{"id":"jE5lxw4ql7XI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we are ready to begin testing our LoRA model on test data that we generate ourselves. We are providing two sentences and the code in order to determine semantic similarity. Afterwards, you can test some yourselves."],"metadata":{"id":"rXx5QTz1HpAQ"}},{"cell_type":"code","source":["# let's look at a specific example\n","# and see what the trained model will do on two samples\n","# that are not necessarily in the training or valiudation data\n","\n","## here is the code to take in two sentences, tokenize them,\n","## and generate model logit outputs. Lastly, we are going to generate predictions\n","## for each classes.\n","def get_preds(sentence1, sentence2, classes=[\"not equivalent\", \"equivalent\"]):\n","  inputs = tokenizer(sentence1,\n","                     sentence2,\n","                     truncation=True,\n","                     padding=\"longest\",\n","                     return_tensors=\"pt\").to(\"cuda\")\n","  with torch.no_grad():\n","    outputs = trainer.model(**inputs).logits\n","    print(outputs)\n","\n","  paraphrased_text = torch.softmax(outputs, dim=1).tolist()[0]\n","  for i in range(len(classes)):\n","      print(f\"{classes[i]}: {int(round(paraphrased_text[i] * 100))}%\")\n","\n","\n","## here are two sentences and we'd like to understand\n","## if the two sentences are equivalent\n","sentence1 = \"Coast redwood trees are the tallest trees on the planet and can grow over 300 feet tall.\"\n","sentence2 = \"The coast redwood trees, which can attain a height of over 300 feet, are the tallest trees on earth.\"\n","\n","## run the get_preds function with these two sentences\n","## code here\n"],"metadata":{"id":"zsa4TJ3kl36D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## go ahead and chose two sentences and\n","## check if they are semantically equivalent\n","## what do you notice about the sentences you choose?\n","\n"],"metadata":{"id":"NQcD0IVziQsz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"DZkVT5b_ri_c"},"execution_count":null,"outputs":[]}]}